import os
import pandas as pd
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
import numpy as np
import re

def load_data(file_path):
    xl = pd.ExcelFile(file_path)

    for i in range(10):
        temp_df = xl.parse(sheet_name='æ•°æ®é›†', header=i)
        if 'b_aab022' in temp_df.columns:
            header_row = i
            break
    else:
        raise ValueError("Error")

    data = xl.parse(sheet_name='æ•°æ®é›†', header=header_row)
    print(data.columns.tolist())

    return data

def load_data_pre(file_path):
    xl = pd.ExcelFile(file_path)

    for i in range(10):
        temp_df = xl.parse(sheet_name='é¢„æµ‹é›†', header=i)
        if 'c_aac181' in temp_df.columns:
            header_row = i
            break
    else:
        raise ValueError("Error")

    data = xl.parse(sheet_name='é¢„æµ‹é›†', header=header_row)
    print(data.columns.tolist())

    return data

def advanced_missing_value_processing(df):
    # ç”Ÿæˆç¼ºå¤±åˆ†ææŠ¥å‘Šï¼ˆå»ºè®®ä¿ç•™æ­¤è¾“å‡ºç”¨äºéªŒè¯ï¼‰
    missing_analysis = df.isna().agg(['sum', 'mean']).T
    missing_analysis.columns = ['ç¼ºå¤±æ•°é‡', 'ç¼ºå¤±æ¯”ä¾‹']
    missing_analysis = missing_analysis.sort_values(by='ç¼ºå¤±æ¯”ä¾‹', ascending=False)
    print("ç¼ºå¤±å€¼åˆ†ææŠ¥å‘Šï¼š\n", missing_analysis.head(10))

    # åˆ é™¤é«˜ç¼ºå¤±ç‡ç‰¹å¾ï¼ˆé˜ˆå€¼è®¾ä¸º70%ï¼‰
    high_missing_cols = missing_analysis[missing_analysis['ç¼ºå¤±æ¯”ä¾‹'] > 0.7].index.tolist()
    if high_missing_cols:
        print(f"ğŸš® åˆ é™¤é«˜ç¼ºå¤±ç‡ç‰¹å¾ï¼š{high_missing_cols}")
        df = df.drop(columns=high_missing_cols)

    # æ¯•ä¸šå¹´ä»½æ¨ç®—ï¼ˆæ ¹æ®æ¯•ä¸šå­¦æ ¡ç±»å‹åˆ¤æ–­æ¯•ä¸šå¹´é¾„ï¼‰
    if 'graduate_school' in df.columns:
        def infer_grad_year(row):
            if pd.notnull(row.get('graduate_year')):
                return row['graduate_year']
            school = str(row.get('c_aac180', ''))
            birth_year = row.get('birth_year', None)
            if pd.isnull(birth_year):
                return np.nan
            # åˆ¤æ–­å­¦æ ¡ç±»å‹
            if any(keyword in school for keyword in ['ä¸­å­¦', 'é«˜ä¸­', 'èŒæ•™', 'èŒé«˜', 'èŒå·¥' ,'æŠ€æ ¡', 'ä¸€ä¸­', 'äºŒä¸­']):
                grad_age = 18
            elif any(keyword in school for keyword in ['å¤§å­¦', 'å­¦é™¢', 'å­¦æ ¡']):
                grad_age = 22
            else:
                grad_age = 20  # é»˜è®¤å€¼ï¼Œé€‚ç”¨äºæ— æ³•åˆ¤æ–­çš„æƒ…å†µ
            return birth_year + grad_age

        df['graduate_year'] = df.apply(infer_grad_year, axis=1)
        df['years_since_grad'] = 2025 - df['graduate_year']

    # åˆ†ç±»å‹ä¸æ•°å€¼å‹å·®å¼‚å¤„ç†
    # ç±»åˆ«å‹ç‰¹å¾å¤„ç†ï¼ˆæ·»åŠ 'Unknown'ç±»åˆ«ï¼‰
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    for col in categorical_cols:
        if col in df.columns:
            df[col] = df[col].astype('category').cat.add_categories(['Unknown']).fillna('Unknown')

    # æ•°å€¼å‹ç‰¹å¾å¤„ç†ï¼ˆMICEç®—æ³•ï¼‰
    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
    if numeric_cols:
        imputer = IterativeImputer(
            max_iter=10,
            random_state=42,
            initial_strategy='median'
        )
        df[numeric_cols] = imputer.fit_transform(df[numeric_cols])

    # å…³é”®ç‰¹å¾éªŒè¯
    assert df['age'].isna().sum() == 0, "å¹´é¾„å­—æ®µä»å­˜åœ¨ç¼ºå¤±ï¼"

    return df

address_map_multi = {
    "è¿œå®‰å¿": ["è¿œå®‰", "æ²³å£", "é¸£å‡¤", "å—é—¨æ‘", "å«˜ç¥–"],
    "äº”å³°åœŸå®¶æ—è‡ªæ²»å¿": ["äº”å³°", "æ¸”æ´‹å…³", "é•¿ä¹åª"],
    "æ©æ–½åœŸå®¶æ—è‹—æ—è‡ªæ²»å·": ["æ©æ–½", "å·´ä¸œ"],
    "é•¿é˜³åœŸå®¶æ—è‡ªæ²»å¿": ["é•¿é˜³", "ç™½æ°åª", "æ¸”å³¡å£", "å¤§å °ä¹¡", "é¾™èˆŸåª"],
    "å…´å±±å¿": ["å…´å±±", "å¤å¤«"],
    "è¥„é˜³å¸‚": ["è¥„é˜³"],
    "ä»™æ¡ƒå¸‚": ["ä»™æ¡ƒ"],
    "æ½œæ±Ÿå¸‚": ["æ½œæ±Ÿ"],
    "å’¸å®å¸‚": ["å’¸å®"],
    "å¤©é—¨å¸‚": ["å¤©é—¨"],
    "æ£é˜³å¸‚": ["æ£é˜³"],
    "å­æ„Ÿå¸‚": ["å­æ„Ÿ"],
    "é„‚å·å¸‚": ["é„‚å·"],
    "éšå·å¸‚": ["éšå·"],
    "ç¥å†œæ¶æ—åŒº": ["ç¥å†œæ¶"],
    "ç§­å½’å¿": ["ç§­å½’", "æ¢…å®¶æ²³", "ä¸¤æ²³å£", "èŒ…åªé•‡", "é™ˆå®¶å†²", "å¹³æ¹–"],
    "ææ±Ÿå¸‚": ["ç™½æ´‹", "ä¸ƒæ˜Ÿå°", "æ¡‚æºªæ¹–", "é¾™æ³‰é•‡", "è‘£å¸‚é•‡", "é©¬å®¶åº—", "ç™¾é‡Œæ´²", "å®‰ç¦å¯º", "ä»™å¥³é•‡", "ä½™å®¶æºª", "å…¬å›­è·¯", "æ°‘ä¸»å¤§é“"],
    "å®œéƒ½å¸‚": ["å®œéƒ½", "å›­æ—å¤§é“", "æåŸ", "é™†åŸ", "é«˜åæ´²", "èµ¤æºªæ²³", "çº¢æ¹–", "çº¢æ˜¥"],
    "è¥¿é™µåŒº": ["è¥¿é™µ", "æ¸¯çª‘", "åŒ—è‹‘è·¯", "çç è·¯", "å‘å±•å¤§é“", "åŸä¸œå¤§é“", "ä¸œæ¹–å¤§é“", "ç æµ·è·¯"],
    "ä¼å®¶å²—åŒº": ["ä¼å®¶å²—", "ä¼ä¸´", "ä¸œå±±å¤§é“", "å¤·é™µå¤§é“", "åˆç›Šè·¯"],
    "çŒ‡äº­åŒº": ["çŒ‡äº­", "ä¸‹é©¬æ§½"],
    "å¤·é™µåŒº": ["å¤·é™µ", "å°æºªå¡”", "ç½—æ²³è·¯", "åˆ†ä¹¡", "è½¦ç«™æ‘", "å¹³äº‘", "é»„èŠ±", "æ¨Ÿæ‘åª", "ä¹å¤©æºª" ,"ä¸‹å ¡åª", "é‚“æ‘ä¹¡", "é¸¦é¹Šå²­", "é›¾æ¸¡æ²³", "ä¸‰æ–—åª", "å¤ªå¹³æºª"],
    "ç‚¹å†›åŒº": ["ç‚¹å†›"],
    "è†é—¨å¸‚": ["è†é—¨", "é’Ÿç¥¥", "æ²™æ´‹"],
    "æ­¦æ±‰å¸‚": ["æ­¦æ±‰"],
    "å½“é˜³å¸‚": ["å½“é˜³", "åé™µ"],
    "è†å·å¸‚": ["è†å·", "å…¬å®‰"],
    "é‡åº†å¸‚": ["é‡åº†"],
    "é»„çŸ³å¸‚": ["é»„çŸ³", "å¤§å†¶"],
    "é»„å†ˆå¸‚": ["éº»åŸ", "é»„å†ˆ", "é»„æ¢…", "æµ æ°´"],
    "é»‘é¾™æ±Ÿ": ["é»‘é¾™æ±Ÿ"],
    "å®‰å¾½": ["å®‰å¾½"],
    "æ²³å—": ["æ²³å—"],
    "ç”˜è‚ƒ": ["ç”˜è‚ƒ"],
    "æ¹–å—": ["æ¹–å—", "é•¿æ²™"],
    "å†…è’™å¤": ["å†…è’™å¤"],
    "å››å·": ["å››å·"],
    "äº‘å—": ["äº‘å—"]
}

def extract_city_or_county(address):
    if pd.isna(address):
        return address
    # å¤šå…³é”®è¯åˆ¤æ–­
    for region, keywords in address_map_multi.items():
        if any(kw in address for kw in keywords):
            return region
    # æ­£åˆ™åŒ¹é…æå–
    match = re.search(r"([^\s]+çœ)?\s*([^å¸‚\s]+å¸‚|[^å¿\s]+å¿)", address)
    if match:
        return match.group(1)
    match = re.search(r"([^å¸‚\s]+å¸‚|[^å¿\s]+å¿|[^ä¹¡\s]+ä¹¡|[^é•‡\s]+é•‡|[^åŒº\s]+åŒº)", address)
    if match:
        return match.group(1)
    return address


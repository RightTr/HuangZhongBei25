import os
import pandas as pd
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
import numpy as np
import re

def load_data(file_path):
    xl = pd.ExcelFile(file_path)

    for i in range(10):
        temp_df = xl.parse(sheet_name='æ•°æ®é›†', header=i)
        if 'b_aab022' in temp_df.columns:
            header_row = i
            break
    else:
        raise ValueError("Error")

    data = xl.parse(sheet_name='æ•°æ®é›†', header=header_row)
    print(data.columns.tolist())

    return data

def load_data_pre(file_path):
    xl = pd.ExcelFile(file_path)

    for i in range(10):
        temp_df = xl.parse(sheet_name='é¢„æµ‹é›†', header=i)
        if 'c_aac181' in temp_df.columns:
            header_row = i
            break
    else:
        raise ValueError("Error")

    data = xl.parse(sheet_name='é¢„æµ‹é›†', header=header_row)
    print(data.columns.tolist())

    return data

def advanced_missing_value_processing(df):
    missing_analysis = df.isna().agg(['sum', 'mean']).T
    missing_analysis.columns = ['ç¼ºå¤±æ•°é‡', 'ç¼ºå¤±æ¯”ä¾‹']
    missing_analysis = missing_analysis.sort_values(by='ç¼ºå¤±æ¯”ä¾‹', ascending=False)
    print("ç¼ºå¤±å€¼åˆ†ææŠ¥å‘Šï¼š\n", missing_analysis.head(10))

    # åˆ é™¤é«˜ç¼ºå¤±ç‡ç‰¹å¾ï¼ˆé˜ˆå€¼è®¾ä¸º70%ï¼‰
    high_missing_cols = missing_analysis[missing_analysis['ç¼ºå¤±æ¯”ä¾‹'] > 0.7].index.tolist()
    if high_missing_cols:
        print(f"ğŸš® åˆ é™¤é«˜ç¼ºå¤±ç‡ç‰¹å¾ï¼š{high_missing_cols}")
        df = df.drop(columns=high_missing_cols)

    # âœ… æ¯•ä¸šå¹´ä»½æ¨ç®—ï¼ˆåº”æ”¾åœ¨ç›¸å…³å­—æ®µå¡«è¡¥åï¼‰
    if 'c_aac180' in df.columns and 'birthday' in df.columns:
        def infer_grad_year(row):
            if pd.notnull(row.get('graduate_year')):
                return row['graduate_year']
            school = str(row.get('c_aac180', ''))
            birth_date = row.get('birthday', None)
            if pd.isnull(birth_date):
                return np.nan
            birth_year = pd.to_datetime(birth_date, errors='coerce').year
            if pd.isnull(birth_year):
                return np.nan
            # åˆ¤æ–­å­¦æ ¡ç±»å‹
            if any(keyword in school for keyword in ['ä¸­å­¦', 'é«˜ä¸­', 'èŒæ•™', 'èŒé«˜', 'èŒå·¥', 'æŠ€æ ¡', 'ä¸€ä¸­', 'äºŒä¸­']):
                grad_age = 18
            elif any(keyword in school for keyword in ['å¤§å­¦', 'å­¦é™¢', 'å­¦æ ¡']):
                grad_age = 22
            else:
                grad_age = 20  # é»˜è®¤å€¼ï¼Œé€‚ç”¨äºæ— æ³•åˆ¤æ–­çš„æƒ…å†µ
            return birth_year + grad_age

        df['graduate_year'] = df.apply(infer_grad_year, axis=1)
        df['years_since_grad'] = 2025 - df['graduate_year']

    # æ•°å€¼å‹ç‰¹å¾å¤„ç†ï¼ˆMICEç®—æ³•ï¼‰
    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
    if numeric_cols:
        imputer = IterativeImputer(
            max_iter=10,
            random_state=42,
            initial_strategy='median'
        )
        df[numeric_cols] = imputer.fit_transform(df[numeric_cols])

    return df

address_map_multi = {
    "è¿œå®‰å¿": ["è¿œå®‰", "æ²³å£", "é¸£å‡¤", "å—é—¨æ‘", "å«˜ç¥–", "æ´‹åª", "æ—§å¿"],
    "äº”å³°åœŸå®¶æ—è‡ªæ²»å¿": ["äº”å³°", "æ¸”æ´‹å…³", "é•¿ä¹åª", "æ¸”å…³é•‡"],
    "æ©æ–½åœŸå®¶æ—è‹—æ—è‡ªæ²»å·": ["æ©æ–½", "å·´ä¸œ", "å»ºå§‹", "é¹¤å³°", "åˆ©å·", "å®£æ©"],
    "é•¿é˜³åœŸå®¶æ—è‡ªæ²»å¿": ["é•¿é˜³", "ç™½æ°åª", "æ¸”å³¡å£", "å¤§å °ä¹¡", "é¾™èˆŸåª"],
    "å…´å±±å¿": ["å…´å±±", "å¤å¤«", "æ˜­å›é•‡", "åŒ—æ–—å°åŒº", "æ¢…è‹‘å°åŒº"],
    "è¥„é˜³å¸‚": ["è¥„é˜³"],
    "ä»™æ¡ƒå¸‚": ["ä»™æ¡ƒ"],
    "æ½œæ±Ÿå¸‚": ["æ½œæ±Ÿ"],
    "å’¸å®å¸‚": ["å’¸å®"],
    "å¤©é—¨å¸‚": ["å¤©é—¨"],
    "æ£é˜³å¸‚": ["æ£é˜³"],
    "å­æ„Ÿå¸‚": ["å­æ„Ÿ", "å­æ˜Œ", "æ±‰å·"],
    "é„‚å·å¸‚": ["é„‚å·"],
    "éšå·å¸‚": ["éšå·"],
    "åå °å¸‚": ["åå °", "éƒ§è¥¿"],
    "ä¸¹æ±Ÿå£å¸‚": ["ä¸¹æ±Ÿå£"],
    "ç¥å†œæ¶æ—åŒº": ["ç¥å†œæ¶"],
    "ç§­å½’å¿": ["ç§­å½’", "æ¢…å®¶æ²³", "ä¸¤æ²³å£", "èŒ…åªé•‡", "é™ˆå®¶å†²", "å¹³æ¹–", "æ°´ç”°å", "æ³„æ»©"],
    "ææ±Ÿå¸‚": ["ææ±Ÿ", "ç™½æ´‹", "ä¸ƒæ˜Ÿå°", "æ¡‚æºªæ¹–", "é¾™æ³‰é•‡", "è‘£å¸‚", "é©¬å®¶åº—", "ç™¾é‡Œæ´²", "å®‰ç¦å¯º", "ä»™å¥³é•‡", "ä½™å®¶æºª", "å…¬å›­è·¯", "æ°‘ä¸»å¤§é“", "å»ºæå¸‚åœº", "ä¹ç•¹æºª", "é©¬å®¶è¡—é“"],
    "å®œéƒ½å¸‚": ["å®œéƒ½", "å›­æ—å¤§é“", "è§£æ”¾ç¤¾åŒº", "æåŸ", "é™†åŸ", "é«˜åæ´²", "èµ¤æºªæ²³", "çº¢æ¹–", "çº¢æ˜¥", "æ¾æœ¨åª", "ä¸­ç¬”ç¤¾åŒº"],
    "è¥¿é™µåŒº": ["è¥¿é™µ", "æ¸¯çª‘", "åŒ—è‹‘è·¯", "çç è·¯", "å‘å±•å¤§é“", "åŸä¸œå¤§é“", "ä¸œæ¹–å¤§é“", "ç æµ·è·¯", "å®œæ˜Œå¸‚"],
    "ä¼å®¶å²—åŒº": ["ä¼å®¶å²—", "ä¼ä¸´", "ä¸œå±±å¤§é“", "å¤·é™µå¤§é“", "åˆç›Šè·¯"],
    "çŒ‡äº­åŒº": ["çŒ‡äº­", "ä¸‹é©¬æ§½"],
    "å¤·é™µåŒº": ["å¤·é™µ", "å°æºªå¡”", "ç½—æ²³è·¯", "åˆ†ä¹¡", "è½¦ç«™æ‘", "å¹³äº‘", "é»„èŠ±", "æ¨Ÿæ‘åª", "ä¹å¤©æºª" ,"ä¸‹å ¡åª", "é‚“æ‘ä¹¡", "é¸¦é¹Šå²­", "é›¾æ¸¡æ²³", "ä¸‰æ–—åª", "å¤ªå¹³æºª"],
    "ç‚¹å†›åŒº": ["ç‚¹å†›"],
    "è†é—¨å¸‚": ["è†é—¨", "é’Ÿç¥¥", "æ²™æ´‹"],
    "æ­¦æ±‰å¸‚": ["æ­¦æ±‰"],
    "å½“é˜³å¸‚": ["å½“é˜³", "åé™µ"],
    "è†å·å¸‚": ["è†å·", "å…¬å®‰", "ç›‘åˆ©"],
    "é‡åº†å¸‚": ["é‡åº†"],
    "é»„çŸ³å¸‚": ["é»„çŸ³", "å¤§å†¶"],
    "é»„å†ˆå¸‚": ["éº»åŸ", "é»„å†ˆ", "é»„æ¢…", "æµ æ°´", "è•²æ˜¥"],
    "é»‘é¾™æ±Ÿçœ": ["é»‘é¾™æ±Ÿ"],
    "å®‰å¾½çœ": ["å®‰å¾½"],
    "æ²³å—çœ": ["æ²³å—"],
    "ç”˜è‚ƒçœ": ["ç”˜è‚ƒ"],
    "å¹¿ä¸œçœ": ["å¹¿ä¸œ", "æ·±åœ³", "å—å±±åŒº"],
    "æ¹–å—çœ": ["æ¹–å—", "é•¿æ²™"],
    "å†…è’™å¤è‡ªæ²»åŒº": ["å†…è’™å¤"],
    "å››å·çœ": ["å››å·"],
    "äº‘å—çœ": ["äº‘å—"],
    "åŒ—äº¬å¸‚": ["åŒ—äº¬"],
}
def extract_city_or_county(address):
    if pd.isna(address):
        return address
    # å¤šå…³é”®è¯åˆ¤æ–­
    for region, keywords in address_map_multi.items():
        if any(kw in address for kw in keywords):
            return region
    # æ­£åˆ™åŒ¹é…æå–
    match = re.search(r"([^\s]+çœ)?\s*([^å¸‚\s]+å¸‚|[^å¿\s]+å¿)", address)
    if match:
        return match.group(1)
    match = re.search(r"([^å¸‚\s]+å¸‚|[^å¿\s]+å¿|[^ä¹¡\s]+ä¹¡|[^é•‡\s]+é•‡|[^åŒº\s]+åŒº)", address)
    if match:
        return match.group(1)
    return address

